 URL : "https://github.com/roest01/docker-speedtest-analyser/issues/4" TITLE : new database system BODY : docker-speedtest-analyser use a result.csv https://raw.githubusercontent.com/roest01/docker-speedtest-analyser/master/data/result.csv as database. the default setting for a speedtest is 1/hour . data addet per speedtest ~35 bytes 35 24 = 840 bytes per day 840 365 = 306.500 bytes 306,5 kilobyte per year scaled up - every 5 minutes the default setting for a speedtest is 12/hour . data addet per speedtest ~35 bytes 35 12 = 420 per hour 420 24 = 10.080 bytes per day 10.080 365 = 3.679.200 bytes 3679,2 kilobyte / 3,6792 mb per year file based database system is acceptable when speedtest scaled up to run every 5 minutes for 75 years to scale? the stored data and allow querying data this project may switch to another database system. sqlite https://www.sqlite.org/index.html - old system ğŸ‘ - reliable ğŸ‘ - local ğŸ‘ lowdb https://github.com/typicode/lowdb - easy to use ğŸ‘ - sql query style ğŸ‘ - local file ğŸ‘ - modern ğŸ‘ - important lowdb doesn't support cluster and may have issues with very large json files ~200mb . ğŸ‘